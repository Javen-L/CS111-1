NAME: Natasha Sarkar
EMAIL: nat41575@gmail.com
ID: 904743795

README


---- **** ---- **** ---- **** ---- **** ---- **** ---- **** ---- **** ---- **** ---- 


FILES:

SortedList.h ... a header file containing interfaces for linked list operations

SortedList.c ... the source for a C source module that compiles cleanly (with no errors 
or warnings), and implements insert, delete, lookup, and length methods for a sorted 
doubly linked list (described in the provided header file, including correct placement of 
pthread_yield calls)

lab2_list.c ...  the source for a C program that compiles cleanly (with no errors or 
warnings), and implements the specified command line options (--threads, --iterations,
 --yield, --sync, --lists), drives one or more parallel threads that do operations on a 
 shared linked list, and reports on the final list and performance

Makefile ... 6 targes:
	default ... compiles the lab2b_list executable  with -Wall and -Wextra options
	tests ... run all specified test cases to generate csv results
	profile ... run tests with profiling tools to generate an execution profiling report
	graphs ... use gnuplot to generate the required graphs
	dist ... create the deliverable tarball
	clean ... delete all programs and output generated by the Makefile

lab2b_list.csv ... contains results for all of the test runs

profile.out ... execution profiling report showing where time was spent in the 
un-partitioned spin-lock implementation

graphs (.png files) created by gnuplot on the above csv data showing
	lab2b_1.png ... throughput vs number of threads for mutex and spin-lock synchronized
	list operations
	lab2b_2.png ... mean time per mutex wait and mean time per operation for mutex-
	synchronized operations
	lab2b_3.png ... successful iterations vs threads for each synchronization method
	lab2b_4.png ... throughput vs number of threads for mutex synchronized partitioned 
	lists
	lab2b_5.png ... throughput vs number of threads for spin-lock-synchronized 
	partitioned lists

lab2b_tests.sh ... a script that runs all specified test cases to generate csv results

lab2b_list.gp ... a script that uses gnuplot to generate the required graphs from csv file


---- **** ---- **** ---- **** ---- **** ---- **** ---- **** ---- **** ---- **** ---- 


QUESTION 2.3.1 - Cycles in the basic list implementation 

In the 1 and 2 thread list tests, there are probably more cycles being spent doing list
operations rather than waiting for locks. If there are only one or two threads, then 
synchronization is not too much of an issue (compared to a larger number of threads), 
and locks become available more quickly. 

Most of the time in high-thread spin-lock tests is spent spinning as threads wait for 
locks to be unlocked. 

Most of the time in high-thread mutex tests is spent in the mutex functions because
they are expensive operations, and with a high number of threads they will occur
often due to the threads all trying to access the same locked resources. 


QUESTION 2.3.2 - Execution Profiling

The lines of code consuming the most cycles in the spin-lock version are

	while (__sync_lock_test_and_set(lock + hash, 1));
	while (__sync_lock_test_and_set(lock + i, 1));

This operation becomes so expensive with larger numbers of threads because the 
increased number of threads are competing for CPU time and, to synchronize these
threads and protect the integrity of the linked lists, the critical sections
of the code are locked with these spin locks. However, this means that many 
cycles are spent just spinning and waiting at the spin lock, wasting CPU cycles.


QUESTION 2.3.3 - Mutex Wait Time

The average lock-wait time rises so dramatically with the number of contending
threads because the threads are forced to wait when multiple threads are trying
to access the same resource. When there is only one thread, the thread doesn't
need to wait for any other threads to finish using the resource, but with more
threads, there is more competition among them and thus a longer wait time for
the lock on the resource.

Completion time per operation rises (less dramatically) with the number of
contending threads because although each thread must wait for the resource when
another thread has locked it, making the completion time rise, but there is
always at least one thread making progress on its part of the program, so it 
is a less dramatic rise.

The wait time per operation goes up faster than the completion time per operation
because each thread has its own timer for the wait time. Thus, when adding each
of these wait times for individual thrads to the total wait time, there is
some overlap among the threads; multiple threads were waiting at the same time.
The completion time, however, does not have overlap as it is run only in the parent
thread. Thus the completion time does not rise as quickly. 


QUESTION 2.3.4 - Performance of Partitioned Lists

As the number of lists increase, the performance improves (the throughput increases).

As the number of lists is further increased, the throughput will continue increasing
until it reaches a limit. The limit is when each element has its own sub list, meaning
that each thread will never have to wait for another one. At this point, increasing
the total number of lists will not have any effect on the throughput of the program.

The suggestion that the throughput of an N-way partitioned list should be equivalent
to the throughput of a single list with fewer (1/N) threads appears to be somewhat
true. The throughput on the graphs of these corresponding points look close but
not exactly the same.